{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac844f4-b53a-4043-8151-cf0fe3ceb66e",
   "metadata": {},
   "source": [
    "# AI201 Programming Assignment 2\n",
    "## Naive Bayes Spam Filter\n",
    "\n",
    "*Submitted by: Mike Allan Nillo*\n",
    "\n",
    "### Table of Contents\n",
    "- Loading of Data\n",
    "- Classifier Construction and Evaluation\n",
    "- Lambda Smoothing\n",
    "- Improving your Classifier\n",
    "- Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e478a8ab-443f-41e9-a8a9-1c65cabaa3e6",
   "metadata": {},
   "source": [
    "### Loading of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d6b9408-c771-462e-bf6b-435deb4543e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Step 1: Load the Labels\n",
    "labels = {}\n",
    "with open('/home/mikeallan/analytics/meng-ai/ai-201/AI201_PA2_Spam_Filter_2SAY23-24/AI201_PA2_Spam_Filter_2SAY23-24/trec06p-ai201/labels', 'r') as f:\n",
    "    for line in f:\n",
    "        label, rel_path = line.strip().split(' ')\n",
    "        filename = os.path.basename(rel_path)\n",
    "        labels[filename] = label\n",
    "\n",
    "\n",
    "# Step 2: Load the Dataset\n",
    "data = []\n",
    "base_path = '/home/mikeallan/analytics/meng-ai/ai-201/AI201_PA2_Spam_Filter_2SAY23-24/AI201_PA2_Spam_Filter_2SAY23-24/trec06p-ai201/data'\n",
    "\n",
    "# Walk through each directory and file in the base_path\n",
    "for dirpath, dirnames, filenames in os.walk(base_path):\n",
    "    for filename in filenames:\n",
    "        file_path = os.path.join(dirpath, filename)\n",
    "        with open(file_path, 'r', errors='ignore') as f:\n",
    "            # Read the file content\n",
    "            content = f.read()\n",
    "            # Get the label from the labels dictionary\n",
    "            label = labels.get(filename, 'unknown')\n",
    "            data.append((content, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f636fe-a00e-4d28-811c-c6f19d428740",
   "metadata": {},
   "source": [
    "### Classifier Construction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3b1e72a-f30f-4bbf-ab32-dd89f98e4818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Assume 'data' is a list of tuples, where each tuple contains the file content and the label\n",
    "random.shuffle(data)\n",
    "\n",
    "# Calculate the index that separates the training data from the test data\n",
    "split_index = int(len(data) * 0.7)\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "train_data = data[:split_index]\n",
    "test_data = data[split_index:]\n",
    "\n",
    "# Separate the texts and the labels\n",
    "texts_train, labels_train = zip(*train_data)\n",
    "texts_test, labels_test = zip(*test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fff278b-5cbd-4d33-b7eb-6e94c09ef537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior probability for spam: 0.7850883819308052\n",
      "Prior probability for ham: 0.21491161806919473\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionaries for spam and ham words\n",
    "spam_words = {}\n",
    "ham_words = {}\n",
    "\n",
    "# Initialize counters for spam and ham documents\n",
    "spam_docs = 0\n",
    "ham_docs = 0\n",
    "\n",
    "# Parse the documents in the training set\n",
    "for text, label in zip(texts_train, labels_train):\n",
    "    # Convert the text to lower case and replace commas and periods with spaces\n",
    "    text = text.lower().replace(',', ' ').replace('.', ' ')\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # Update the appropriate dictionaries\n",
    "    if label == 'spam':\n",
    "        spam_docs += 1\n",
    "        for word in words:\n",
    "            if word.isalpha():  # Check if the word contains only alphabetic characters\n",
    "                spam_words[word] = spam_words.get(word, 0) + 1\n",
    "    else:\n",
    "        ham_docs += 1\n",
    "        for word in words:\n",
    "            if word.isalpha():  # Check if the word contains only alphabetic characters\n",
    "                ham_words[word] = ham_words.get(word, 0) + 1\n",
    "\n",
    "# Form the vocabulary of unique words in the training data\n",
    "vocabulary = set(spam_words.keys()).union(set(ham_words.keys()))\n",
    "\n",
    "# Count the total number of documents\n",
    "total_docs = spam_docs + ham_docs\n",
    "\n",
    "# Calculate and report the prior probabilities for spam and ham\n",
    "prior_spam = spam_docs / total_docs\n",
    "prior_ham = ham_docs / total_docs\n",
    "\n",
    "print(f'Prior probability for spam: {prior_spam}')\n",
    "print(f'Prior probability for ham: {prior_ham}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97963f74-50e5-443a-add3-9c3f4b6d3dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of words in spam and ham documents\n",
    "total_spam_words = sum(spam_words.values())\n",
    "total_ham_words = sum(ham_words.values())\n",
    "\n",
    "# Calculate the total number of words in the vocabulary\n",
    "total_words = len(vocabulary)\n",
    "\n",
    "# Initialize dictionaries for spam and ham probabilities\n",
    "spam_probs = {}\n",
    "ham_probs = {}\n",
    "\n",
    "# Calculate the word probabilities for spam and ham\n",
    "for word in vocabulary:\n",
    "    spam_probs[word] = (spam_words.get(word, 0) + 1) / (total_spam_words + total_words)\n",
    "    ham_probs[word] = (ham_words.get(word, 0) + 1) / (total_ham_words + total_words)\n",
    "\n",
    "# Define a function to classify a text as spam or ham\n",
    "def classify(text):\n",
    "    # Convert the text to lower case and replace commas and periods with spaces\n",
    "    text = text.lower().replace(',', ' ').replace('.', ' ')\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # Initialize the spam and ham probabilities with the prior probabilities\n",
    "    spam_prob = prior_spam\n",
    "    ham_prob = prior_ham\n",
    "    # Update the probabilities for each word in the text\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            spam_prob *= spam_probs[word]\n",
    "            ham_prob *= ham_probs[word]\n",
    "    # Return the class with the highest probability\n",
    "    return 'spam' if spam_prob > ham_prob else 'ham'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b174463-9da5-4209-82f9-81b6a03eb49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.41394201110425666\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters for correct and total predictions\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Classify the documents in the test set\n",
    "for text, true_label in zip(texts_test, labels_test):\n",
    "    # Classify the text\n",
    "    predicted_label = classify(text)\n",
    "    # Update the counters\n",
    "    if predicted_label == true_label:\n",
    "        correct_predictions += 1\n",
    "    total_predictions += 1\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fab3ee4e-95d4-4d60-a74e-0657e271c622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7840144852560786\n",
      "Recall: 0.3426407415781144\n"
     ]
    }
   ],
   "source": [
    "def calculate_precision_recall(predictions, labels):\n",
    "    # Initialize counters for true positives, false positives, and false negatives\n",
    "    tp = fp = fn = 0\n",
    "\n",
    "    # Count the true positives, false positives, and false negatives\n",
    "    for predicted, true in zip(predictions, labels):\n",
    "        if predicted == 'spam':\n",
    "            if true == 'spam':\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        elif true == 'spam':\n",
    "            fn += 1\n",
    "\n",
    "    # Calculate and return the precision and recall\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    return precision, recall\n",
    "\n",
    "# Use the function to calculate the precision and recall\n",
    "predictions = [classify(text) for text in texts_test]\n",
    "precision, recall = calculate_precision_recall(predictions, labels_test)\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e810ade-37b5-4bca-aef0-109dcd8e36d1",
   "metadata": {},
   "source": [
    "### Lambda Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d8fccfb-497d-4c12-b233-2de07817f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to classify a text as spam or ham with lambda smoothing\n",
    "def classify_lambda(text, lambda_value):\n",
    "    # Convert the text to lower case and replace commas and periods with spaces\n",
    "    text = text.lower().replace(',', ' ').replace('.', ' ')\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # Initialize the spam and ham probabilities with the prior probabilities\n",
    "    spam_prob = prior_spam\n",
    "    ham_prob = prior_ham\n",
    "    # Update the probabilities for each word in the text\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            spam_prob *= spam_probs.get(word, lambda_value / (total_spam_words + total_words * lambda_value))\n",
    "            ham_prob *= ham_probs.get(word, lambda_value / (total_ham_words + total_words * lambda_value))\n",
    "    # Return the class with the highest probability\n",
    "    return 'spam' if spam_prob > ham_prob else 'ham'\n",
    "\n",
    "# Define a function to apply the classifier for different values of lambda\n",
    "def apply_classifier_lambda(texts, labels, lambda_values):\n",
    "    for lambda_value in lambda_values:\n",
    "        # Classify the texts and calculate the precision and recall\n",
    "        predictions = [classify_lambda(text, lambda_value) for text in texts]\n",
    "        precision, recall = calculate_precision_recall(predictions, labels)\n",
    "        print(f'Lambda: {lambda_value}, Precision: {precision}, Recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34510fc7-d27a-44a6-858f-4acc3dc09a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 10.0, Precision: 0.7840144852560786, Recall: 0.3426407415781144\n",
      "Lambda: 1.0, Precision: 0.7840144852560786, Recall: 0.3426407415781144\n",
      "Lambda: 0.5, Precision: 0.7840144852560786, Recall: 0.3426407415781144\n",
      "Lambda: 0.1, Precision: 0.7840144852560786, Recall: 0.3426407415781144\n",
      "Lambda: 0.005, Precision: 0.7840144852560786, Recall: 0.3426407415781144\n"
     ]
    }
   ],
   "source": [
    "# Apply the classifier for different values of lambda\n",
    "apply_classifier_lambda(texts_test, labels_test, [2.0, 1.0, 0.5, 0.1, 0.005])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a184210-85cf-470c-ae69-f950169a50a7",
   "metadata": {},
   "source": [
    "### Improving Your Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8f16cfc-2a99-4946-82b3-661905b0ddd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 200 informative words for spam:\n",
      "unicode\n",
      "dances\n",
      "usnews\n",
      "watsun\n",
      "libungif\n",
      "libxaw\n",
      "townson\n",
      "dance\n",
      "sunbird\n",
      "zimpleware\n",
      "lcs\n",
      "ecd\n",
      "diversityinc\n",
      "kp\n",
      "sundance\n",
      "libpcap\n",
      "nanp\n",
      "sybase\n",
      "dialing\n",
      "higdon\n",
      "vetmed\n",
      "afs\n",
      "sandman\n",
      "judith\n",
      "libpng\n",
      "libpopt\n",
      "libbonobox\n",
      "libvorbisfile\n",
      "libmikmod\n",
      "libgpm\n",
      "libdb\n",
      "libfmpich\n",
      "libmemintercept\n",
      "libartscbackend\n",
      "libvorbisenc\n",
      "libartswavplayobject\n",
      "libutempter\n",
      "libtix\n",
      "libgnome\n",
      "libsnmp\n",
      "libsensors\n",
      "libfreetype\n",
      "libogg\n",
      "libuser\n",
      "libao\n",
      "libdns\n",
      "libgtkxmhtml\n",
      "libgtkxtbin\n",
      "libucdmibs\n",
      "libisc\n",
      "libmozz\n",
      "libgnomesupport\n",
      "libesd\n",
      "librep\n",
      "libtcl\n",
      "libesmtp\n",
      "libzvt\n",
      "libglu\n",
      "libgdbm\n",
      "libconsole\n",
      "libaspell\n",
      "libcap\n",
      "libscrollkeeper\n",
      "libbeomap\n",
      "libmm\n",
      "libmp\n",
      "libmcop\n",
      "libunicode\n",
      "libmysqlclient\n",
      "libtiff\n",
      "libswigpy\n",
      "libgnomevfs\n",
      "libefs\n",
      "liborbitutil\n",
      "libgd\n",
      "libgnomecanvaspixbuf\n",
      "libxdelta\n",
      "libxpcom\n",
      "libnewt\n",
      "libmpich\n",
      "libpgm\n",
      "libmenu\n",
      "libsasl\n",
      "libltdl\n",
      "libimlib\n",
      "libpanel\n",
      "libaudiofile\n",
      "libostyle\n",
      "libpspell\n",
      "libldap\n",
      "libedsio\n",
      "libncurses\n",
      "libmng\n",
      "libeel\n",
      "libnssckbi\n",
      "libslang\n",
      "libgtksuperwin\n",
      "libcfont\n",
      "libxml\n",
      "libz\n",
      "libgtkhtml\n",
      "libogrove\n",
      "liborbit\n",
      "libpcreposix\n",
      "libswigtcl\n",
      "libpcprofile\n",
      "libmozjs\n",
      "libosp\n",
      "libgnorbagtk\n",
      "libctutils\n",
      "libglut\n",
      "liblockdev\n",
      "libxpistub\n",
      "libiiop\n",
      "libexslt\n",
      "libpmpich\n",
      "libartsdsp\n",
      "libmemusage\n",
      "libartsc\n",
      "libform\n",
      "libjsj\n",
      "libgnomeprint\n",
      "libjpeg\n",
      "libgdkcardimage\n",
      "libvorbis\n",
      "libgl\n",
      "libguilereadline\n",
      "libucdagent\n",
      "libguile\n",
      "libgnomeui\n",
      "libesddsp\n",
      "liboaf\n",
      "libpbm\n",
      "libospgrove\n",
      "libcrack\n",
      "libghttp\n",
      "liborbitcosnaming\n",
      "libcapplet\n",
      "libartsflow\n",
      "libbonobo\n",
      "libreadline\n",
      "libglade\n",
      "libctgeneric\n",
      "libsegfault\n",
      "libxslt\n",
      "libgkgfx\n",
      "libobjc\n",
      "librsvg\n",
      "libcurl\n",
      "libpnm\n",
      "libfam\n",
      "libhistory\n",
      "liblber\n",
      "libgmp\n",
      "libnjamd\n",
      "libgtop\n",
      "libexpat\n",
      "libalchemist\n",
      "libttf\n",
      "libppm\n",
      "libgtkembedmoz\n",
      "libnautilus\n",
      "libxsltbreakpoint\n",
      "libxmms\n",
      "libbproc\n",
      "libgal\n",
      "libtk\n",
      "libgnorba\n",
      "libmsgbaseutil\n",
      "infi\n",
      "libtreeplayer\n",
      "libeg\n",
      "libpgsql\n",
      "libhistpainter\n",
      "libtable\n",
      "libxtst\n",
      "librgl\n",
      "libdps\n",
      "librfio\n",
      "libqt\n",
      "libproof\n",
      "libxext\n",
      "libxrender\n",
      "libxmu\n",
      "libgpad\n",
      "libxt\n",
      "libnew\n",
      "libmc\n",
      "libpty\n",
      "libxrx\n",
      "libxfont\n",
      "libhbook\n",
      "libsm\n",
      "libxp\n",
      "libmagick\n",
      "libgeompainter\n",
      "libxft\n",
      "libegpythia\n",
      "librint\n",
      "libdpstk\n",
      "\n",
      "Top 200 informative words for ham:\n",
      "depicting\n",
      "highroad\n",
      "apsmsgs\n",
      "luisa\n",
      "telaviv\n",
      "manageability\n",
      "mrna\n",
      "ops\n",
      "clipart\n",
      "qualfienders\n",
      "dhcpd\n",
      "councilors\n",
      "clienta\n",
      "ttdbserverd\n",
      "futuro\n",
      "ldm\n",
      "brent\n",
      "nanorods\n",
      "seemo\n",
      "apcircuits\n",
      "shepler\n",
      "grp\n",
      "nfs\n",
      "snmp\n",
      "opaque\n",
      "fibre\n",
      "itcz\n",
      "nemacolin\n",
      "asl\n",
      "threadtail\n",
      "lexmark\n",
      "ssd\n",
      "exportfs\n",
      "gibson\n",
      "methodist\n",
      "forecaster\n",
      "microfinanzas\n",
      "schlemann\n",
      "rtgs\n",
      "seattleplus\n",
      "emc\n",
      "wx\n",
      "spaceguardindia\n",
      "asci\n",
      "parkerbros\n",
      "ciac\n",
      "activision\n",
      "consistancy\n",
      "cwgf\n",
      "fh\n",
      "tdatetime\n",
      "saddlebags\n",
      "mennonites\n",
      "getdeviceinfo\n",
      "redirector\n",
      "lanl\n",
      "hertz\n",
      "willow\n",
      "corbett\n",
      "monsoon\n",
      "exhort\n",
      "madd\n",
      "stoping\n",
      "skimmer\n",
      "importer\n",
      "onslow\n",
      "rainfinity\n",
      "trondhjem\n",
      "slimmer\n",
      "nazonazonazonazonazonazonazonazonazonazonazonazonazonazonazonazonazonazonazo\n",
      "isdn\n",
      "filehandle\n",
      "duhons\n",
      "layoutcommit\n",
      "ngfyngfyngfyngfyngfyngfyngfyngfyngfyngfyngfyngfyngfyngfyngfyngfyngfyngfyngfy\n",
      "oqbdoqbdoqbdoqbdoqbdoqbdoqbdoqbdoqbdoqbdoqbdoqbdoqbdoqbdoqbdoqbdoqbdoqbdoqbd\n",
      "dicer\n",
      "ggriderpnfs\n",
      "setclientid\n",
      "finboard\n",
      "jovita\n",
      "nqbwnqbwnqbwnqbwnqbwnqbwnqbwnqbwnqbwnqbwnqbwnqbwnqbwnqbwnqbwnqbwnqbwnqbwnqbw\n",
      "cthulu\n",
      "ngbyngbyngbyngbyngbyngbyngbyngbyngbyngbyngbyngbyngbyngbyngbyngbyngbyngbyngby\n",
      "lambdalogic\n",
      "ccdaude\n",
      "suo\n",
      "ausoftware\n",
      "hazus\n",
      "nhvjnhvjnhvjnhvjnhvjnhvjnhvjnhvjnhvjnhvjnhvjnhvjnhvjnhvjnhvjnhvjnhvjnhvjnhvj\n",
      "klokka\n",
      "sfs\n",
      "mauritius\n",
      "skreiv\n",
      "twg\n",
      "abm\n",
      "myedm\n",
      "zib\n",
      "exmouth\n",
      "getdevicelist\n",
      "tmtymailu\n",
      "delegation\n",
      "romanow\n",
      "bakeathon\n",
      "myklebust\n",
      "freyberg\n",
      "writable\n",
      "dup\n",
      "accesses\n",
      "neps\n",
      "atr\n",
      "fmp\n",
      "layoutreturn\n",
      "mennonite\n",
      "riots\n",
      "alumniathlete\n",
      "talpey\n",
      "ufs\n",
      "hawaiians\n",
      "citi\n",
      "unios\n",
      "myinks\n",
      "extents\n",
      "idempotency\n",
      "kt\n",
      "msw\n",
      "pawlowski\n",
      "sher\n",
      "davidnoveck\n",
      "ginsparg\n",
      "nansnansnansnansnansnansnansnansnansnansnansnansnansnansnansnansnansnansnans\n",
      "benny\n",
      "hopkinton\n",
      "enterasys\n",
      "faure\n",
      "cyclone\n",
      "mclarty\n",
      "broderbund\n",
      "layouts\n",
      "tropical\n",
      "iscsi\n",
      "xmetohorizontalpixels\n",
      "mfr\n",
      "amv\n",
      "halevy\n",
      "backends\n",
      "getattr\n",
      "pdl\n",
      "tooltalk\n",
      "julo\n",
      "tipmagazine\n",
      "niu\n",
      "cyclones\n",
      "welch\n",
      "netapp\n",
      "pfs\n",
      "pnfsds\n",
      "rddp\n",
      "tyce\n",
      "clientid\n",
      "jtwc\n",
      "medlicott\n",
      "rdma\n",
      "nwbynwbynwbynwbynwbynwbynwbynwbynwbynwbynwbynwbynwbynwbynwbynwbynwbynwbynwby\n",
      "trond\n",
      "pnfsd\n",
      "frejya\n",
      "layoutget\n",
      "grider\n",
      "indiaphonecards\n",
      "floyd\n",
      "kysrkysrkysrkysrkysrkysrkysrkysrkysrkysrkysrkysrkysrkysrkysrkysrkysrkysr\n",
      "tues\n",
      "xmtexmtexmtexmtexmtexmtexmtexmtexmtexmtexmtexmtexmtexmtexmtexmtexmtexmte\n",
      "hildebrand\n",
      "satran\n",
      "diwa\n",
      "noveck\n",
      "fsid\n",
      "concall\n",
      "atari\n",
      "stateid\n",
      "nhc\n",
      "nane\n",
      "mpio\n",
      "kts\n",
      "delegations\n",
      "megacenter\n",
      "umsm\n",
      "proxying\n",
      "pikes\n",
      "akwbaacsaqaaraeaakwbaacsaqaaraeaakwbaacsaqaaraeaakwbaacsaqaaraeaakwbaacsaqaa\n",
      "aacsaqaaraeaakwbaacsaqaaraeaakwbaacsaqaaraeaakwbaacsaqaaraeaakwbaacsaqaaraea\n",
      "aqaaraeaakwbaacsaqaaraeaakwbaacsaqaaraeaakwbaacsaqaaraeaakwbaacsaqaaraeaakwb\n",
      "raeaakwbaacsaqaaraeaakwbaacsaqaaraeaakwbaacsaqaaraeaakwbaacsaqaaraeaakwbaacs\n",
      "panasas\n",
      "garth\n",
      "pnfs\n",
      "nwbwnwbwnwbwnwbwnwbwnwbwnwbwnwbwnwbwnwbwnwbwnwbwnwbwnwbwnwbwnwbwnwbwnwbwnwbw\n",
      "zmptzmptzmptzmptzmptzmptzmptzmptzmptzmptzmptzmptzmptzmptzmptzmptzmptzmptzmpt\n"
     ]
    }
   ],
   "source": [
    "# Calculate the ratio of the spam probability to the ham probability for each word\n",
    "ratios = {word: spam_probs[word] / ham_probs[word] for word in vocabulary}\n",
    "\n",
    "# Sort the words by the ratio in descending order\n",
    "sorted_words = sorted(ratios.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 200 informative words for spam\n",
    "print('Top 200 informative words for spam:')\n",
    "for word, ratio in sorted_words[:200]:\n",
    "    print(word)\n",
    "\n",
    "# Print the top 200 informative words for ham\n",
    "print('\\nTop 200 informative words for ham:')\n",
    "for word, ratio in sorted_words[-200:]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "776317ef-0a12-4d17-a6e4-d29d1ba34efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0.1, Precision: 0.7806244995996797, Recall: 0.9919737734569297\n"
     ]
    }
   ],
   "source": [
    "# Get the top 200 informative words for spam and ham\n",
    "top_spam_words = [word for word, ratio in sorted_words[:200]]\n",
    "top_ham_words = [word for word, ratio in sorted_words[-200:]]\n",
    "\n",
    "# Combine the top spam and ham words into a smaller vocabulary\n",
    "smaller_vocabulary = set(top_spam_words + top_ham_words)\n",
    "\n",
    "# Define a function to classify a text as spam or ham with lambda smoothing and a smaller vocabulary\n",
    "def classify_lambda_small_vocab(text, lambda_value):\n",
    "    # Convert the text to lower case and replace commas and periods with spaces\n",
    "    text = text.lower().replace(',', ' ').replace('.', ' ')\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # Initialize the spam and ham probabilities with the prior probabilities\n",
    "    spam_prob = prior_spam\n",
    "    ham_prob = prior_ham\n",
    "    # Update the probabilities for each word in the text\n",
    "    for word in words:\n",
    "        if word in smaller_vocabulary:\n",
    "            spam_prob *= spam_probs.get(word, lambda_value / (total_spam_words + total_words * lambda_value))\n",
    "            ham_prob *= ham_probs.get(word, lambda_value / (total_ham_words + total_words * lambda_value))\n",
    "    # Return the class with the highest probability\n",
    "    return 'spam' if spam_prob > ham_prob else 'ham'\n",
    "\n",
    "# Apply the classifier for the best value of lambda\n",
    "best_lambda = 0.1  # Replace with the best value of lambda found above\n",
    "predictions = [classify_lambda_small_vocab(text, best_lambda) for text in texts_test]\n",
    "precision, recall = calculate_precision_recall(predictions, labels_test)\n",
    "print(f'Lambda: {best_lambda}, Precision: {precision}, Recall: {recall}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
